{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведите по 2 примера, когда лучше максимизировать Precision, а когда Recall.\n",
    "\n",
    "1) Организация - сеть розничных магазинов. Она проводит эксперимент с новым товаром. В итоге мы классифицируем магазины следующим образом: 1 - магазин продаст новинку, 0 - магазин не продаст новинку.\n",
    "Если у компании текущая стратегия заточена на быстрое развитие, не смотря, на возможно замороженные деньги, или товар высокомаржинальный и не дорогой, то лучше максимизировать Recall, дабы охватить максимальное количество клиентов новинкой.\n",
    "Если у компании текущая стратегия заточена на оптимизацию с минимальными вложениями в новые проекты, или же товар дорогой и низкомаржинальный, то лучше максимизировать Precision, дабы был меньше шанс заморозить деньги в товаре.\n",
    "\n",
    "2) Большой адронный коллайдер - проводятся эксперементы по столкновению различных частиц. В результате столкновения проводится измерение параметров у частиц, которые вылетают из столкновений. Наша модель на основании параметров вылетевшей частицы говорит: 1 - неивестная(новая) частица, 0 - известная частица. Лучше максимизировать Recall дабы не пропустить возможных новых открытий :)\n",
    "\n",
    "3) Камера фиксации правонарушений - на основании видеопотока определяет совершает ли правонарушение водитель. Классификация 1 - правонарушение зафиксировано, 0 - правонарушение не зафиксировано.\n",
    "Если городу нужно больше денег, и мы не заморачиваемся по поводу того, что людям нужно пройти 7 кругов ада,чтобы доказать отсутствие правонарушения - мы максимизируем Recall. Если же мы все таки заботимся о гражданах и не хотим, чтоб им прилетали случайные штрафы - максимизируем Precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почему мы используем F-меру, почему, например, нельзя просто взять среднее от Precision и Recall?\n",
    "\n",
    "Когда один из показателей будет низким, например один = 0.1, второй = 0.9,\n",
    "то в случае ср. арефметического мы получим 0.45, а F-мера будет равна 0.18.\n",
    "Т.о. мы страхуемся от сильно низких значениях в одном параметре и в другом, что может показывать, что модель переобучена.\n",
    "Плюс можно варьировать показатель beta, что позволит сосредоточиться на одной из двух мер в большей степени."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #взял тестовый пример, чтобы на живых данных посчитать\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training_project_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for colname in ['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']:\n",
    "    df[colname] = df[colname].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['EDUCATION'] == '0', 'EDUCATION'] = df['EDUCATION'].mode()[0]\n",
    "df.loc[df['MARRIAGE'] == '0', 'MARRIAGE'] = df['MARRIAGE'].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ID'] = df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['IS_MALE'] = df['SEX'].map({'1':'1', '2':'0'}).astype(int)\n",
    "for cat_colname in df.select_dtypes(include='object').columns[1:]:\n",
    "    df = pd.concat([df, pd.get_dummies(df[cat_colname], prefix=cat_colname)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 61), (2000, 61))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(columns='NEXT_MONTH_DEFAULT')\n",
    "y = df['NEXT_MONTH_DEFAULT']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=2)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tree = DecisionTreeClassifier(random_state=1)\n",
    "\n",
    "param_grid=[{\n",
    "             'max_depth': range(1, 7), \n",
    "             'min_samples_leaf': range(1, 7), \n",
    "            }]\n",
    "\n",
    "grid_search = GridSearchCV(tree, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "tree = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89156047, 0.10843953],\n",
       "       [0.89156047, 0.10843953],\n",
       "       [0.89156047, 0.10843953],\n",
       "       [0.89156047, 0.10843953],\n",
       "       [0.79866332, 0.20133668]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train = tree.predict(X_train)\n",
    "pred_test = tree.predict(X_test)\n",
    "\n",
    "pred_proba_test = tree.predict_proba(X_test)\n",
    "pred_proba_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализовать функции для подсчета Accuracy, Precision, Recall, F-score, которые на вход принимают y_true (истинные значения), y_pred (предсказанные значения), а на выход дается метрика."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x, y):\n",
    "    def pred2(z): #Скорее всего можно проще, но с df работать удобнее)\n",
    "        z = pd.DataFrame(z).reset_index()\n",
    "        z.drop('index', axis= 1, inplace=True)\n",
    "        z.columns=['value']\n",
    "        return z\n",
    "    x = pred2(x)\n",
    "    y = pred2(y)\n",
    "    if len(x) != len(y):\n",
    "        raise Exception('x and y has different len')\n",
    "    else:\n",
    "        return x.merge(y, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    y = pred(y_true, y_pred)\n",
    "    return len(y[y['value_x'] == y['value_y']]) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred, value=1): # value - значение для которого будем считать соответствующую метрику\n",
    "    y = pred(y_true, y_pred)\n",
    "    tp = len(y[(y['value_x'] == value) & (y['value_y'] == value)])\n",
    "    fp = len(y[(y['value_x'] != value) & (y['value_y'] == value)])\n",
    "    return  tp / (tp + fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred, value=1): # value - значение для которого будем считать соответствующую метрику\n",
    "    y = pred(y_true, y_pred)\n",
    "    tp = len(y[(y['value_x'] == value) & (y['value_y'] == value)])\n",
    "    fn = len(y[(y['value_x'] == value) & (y['value_y'] != value)])\n",
    "    return  tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_score(y_true, y_pred, value=1, beta=1): # value - значение для которого будем считать соответствующую метрику\n",
    "    prec = precision(y_true, y_pred, value)\n",
    "    rec = recall(y_true, y_pred, value)\n",
    "    return  (1 + beta ** 2) * prec * rec / (beta ** 2 * prec + rec )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89      1587\n",
      "           1       0.62      0.33      0.43       413\n",
      "\n",
      "    accuracy                           0.82      2000\n",
      "   macro avg       0.73      0.64      0.66      2000\n",
      "weighted avg       0.80      0.82      0.80      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n",
      "Precision: 0.62\n",
      "Recall: 0.33\n",
      "F-score: 0.43\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy(y_test, pred_test):.2f}')\n",
    "print(f'Precision: {precision(y_test, pred_test):.2f}')\n",
    "print(f'Recall: {recall(y_test, pred_test):.2f}')\n",
    "print(f'F-score: {f_score(y_test, pred_test):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.82\n",
      "Precision: 0.84\n",
      "Recall: 0.95\n",
      "F-score: 0.89\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {accuracy(y_test, pred_test):.2f}')\n",
    "print(f'Precision: {precision(y_test, pred_test, value=0):.2f}')\n",
    "print(f'Recall: {recall(y_test, pred_test, value=0):.2f}')\n",
    "print(f'F-score: {f_score(y_test, pred_test, value=0):.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
